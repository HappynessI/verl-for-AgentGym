
set -e

# é…ç½®å‚æ•°
MODEL_PATH="/models/Qwen3-4B-Thinking-2507"
PORT=8000
TP_SIZE=1  
DP_SIZE=8
MAX_MODEL_LEN=32768
GPU_MEM_UTIL=0.95
HOST="0.0.0.0"
LOG_FILE="/agent_distill/logs/vllm_server_$(date +%Y%m%d_%H%M%S).log"

echo "ðŸš€ å¯åŠ¨vLLMæœåŠ¡ [$(date)]"
echo "æ¨¡åž‹è·¯å¾„: $MODEL_PATH"
echo "å¼ é‡å¹¶è¡Œ: $TP_SIZE"
echo "æœ€å¤§ä¸Šä¸‹æ–‡: $MAX_MODEL_LEN"
echo "GPUæ˜¾å­˜åˆ©ç”¨çŽ‡: $GPU_MEM_UTIL"
echo "æœåŠ¡åœ°å€: http://$HOST:$PORT"
echo "æ—¥å¿—æ–‡ä»¶: $LOG_FILE"
echo "----------------------------------------"

# è®¾ç½®CUDA_VISIBLE_DEVICESä¸ºæ‰€æœ‰å¡
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

# å¯åŠ¨vLLMæœåŠ¡ (åŽå°è¿è¡Œ)
nohup vllm serve "$MODEL_PATH" \
  --served-model-name "qwen3" \
  --tensor-parallel-size $TP_SIZE \
  --data-parallel-size $DP_SIZE     \
  --max-model-len $MAX_MODEL_LEN \
  --gpu-memory-utilization $GPU_MEM_UTIL \
  --host $HOST \
  --port $PORT \
  --disable-log-requests \
  --max-num-seqs 128 \
  --enable-chunked-prefill \
  --max-log-len 0 \
  > "$LOG_FILE" 2>&1 &

SERVER_PID=$!
